{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe49d3b1",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Environment for Option Hedging\n",
    "---\n",
    "\n",
    "<p>\n",
    "This notebook aims to create an OpenAI Gym environment which simulates trading\n",
    "in an option hedging scenario. The model will observe the market (asset price \n",
    "path generated by GBM) and will decide on a suitable action of buying / selling\n",
    "the underlying asset in order to hedge dynamically.\n",
    "</p>\n",
    "\n",
    "---\n",
    "Key Components:\n",
    "- Environment: Simulated GBM price paths\n",
    "- Action: Decided by the model based on the condition of environment in order to \n",
    "produce an optimal hedge\n",
    "- State: Stock price, time to maturity, current hedge, portfolio value\n",
    "- Reward: function to grade the action taken by model against the optimal action\n",
    "(optimal action: BSM; reward function: P&L/Negative Hedging error/ Sharpe Ratio etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455ef346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "from scipy.stats import norm\n",
    "sys.path.append('../utils')\n",
    "from gbm import simulate_gbm_path\n",
    "from bsm import bs_call_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35b845c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HedgingEnv(gymnasium.Env):\n",
    "    \"\"\"\n",
    "    Custom Gymnasium environment for option hedging using RL.\n",
    "    The agent learns to adjust its hedge dynamically to minimize risk/cost.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    def __init__(self, S0=100, K=100, mu=0.05, sigma=0.2, T=1.0, steps=252, cost=0.001, reward_type=\"hedge_error\"):\n",
    "        super(HedgingEnv, self).__init__()\n",
    "\n",
    "        self.S0, self.K, self.mu, self.sigma, self.T, self.steps, self.cost, self.reward_type = S0, K, mu, sigma, T, steps, cost, reward_type\n",
    "        self.dt = T / steps\n",
    "        self.returns_window = []\n",
    "\n",
    "        # Observation space: [normalized price, time to maturity, current hedge]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0, 0.0, -1.0], dtype=np.float32),\n",
    "            high=np.array([2.0, 1.0, 1.0], dtype=np.float32),\n",
    "        )\n",
    "\n",
    "        # Action space: continuous scalar (hedge adjustment)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        self.reset(seed=None)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Generate new path through GBM\n",
    "        self.S = simulate_gbm_path(S0=self.S0, mu=self.mu, sigma=self.sigma, dt=self.dt, steps=self.steps)\n",
    "        self.t = 0\n",
    "        self.hedge = 0.0\n",
    "        self.cash = 0.0\n",
    "        self.portfolio_value = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        normalised_price = self.S[self.t] / self.S0\n",
    "        time_to_maturity = 1.0 - (self.t / self.steps)\n",
    "        return np.array([normalised_price, time_to_maturity, self.hedge], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = float(np.clip(action[0], -1.0, 1.0))\n",
    "        prev_hedge = self.hedge\n",
    "        self.hedge = np.clip(prev_hedge + action * 0.1, -1.0, 1.0)\n",
    "\n",
    "        delta_change = self.hedge - prev_hedge\n",
    "        self.cash += delta_change * self.S[self.t] - abs(delta_change) * self.cost * self.S[self.t]\n",
    "        self.cash *= np.exp(self.mu * self.dt)\n",
    "\n",
    "        self.t += 1\n",
    "        terminated = self.t >= self.steps - 1\n",
    "\n",
    "        tau = self.T - self.t * self.dt\n",
    "        option_value = bs_call_price(S=self.S[self.t], K=self.K, r=self.mu, sigma=self.sigma, T=tau)\n",
    "        portfolio_value = option_value - self.hedge * self.S[self.t] + self.cash\n",
    "        reward = self._calculate_reward(portfolio_value=portfolio_value)\n",
    "        self.portfolio_value = portfolio_value\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, terminated, False, {}\n",
    "\n",
    "    def _calculate_reward(self, portfolio_value):\n",
    "        if self.reward_type == \"pnl\":\n",
    "            return portfolio_value - self.portfolio_value\n",
    "\n",
    "        elif self.reward_type == \"hedge_error\":\n",
    "            return -(portfolio_value - self.portfolio_value)**2\n",
    "\n",
    "        elif self.reward_type == \"sharpe\":\n",
    "            self.returns_window.append(portfolio_value - self.portfolio_value)\n",
    "            if len(self.returns_window) > 20:\n",
    "                vol = np.std(self.returns_window[-20:])\n",
    "                return (portfolio_value - self.portfolio_value) / (vol + 1e-6)\n",
    "            else:\n",
    "                return portfolio_value - self.portfolio_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362779e",
   "metadata": {},
   "source": [
    "## Reward Function Design\n",
    "---\n",
    "\n",
    "The environment is now complete and ready for training.  \n",
    "However, the reward design is **crucial**, since it determines what behaviour the RL agent learns.\n",
    "\n",
    "We'll experiment with three different reward formulations:\n",
    "\n",
    "1. **P&L-Based Reward (Profit/Loss)**  \n",
    "   - Encourages maximizing raw portfolio gains.  \n",
    "   - Reward:  \n",
    "     $$\n",
    "     r_t = V_{t+1} - V_t\n",
    "     $$\n",
    "   - Useful for early debugging or when modelling profit-seeking behaviour.\n",
    "\n",
    "2. **Hedging Error Reward (Stability)**  \n",
    "   - Encourages minimizing fluctuations in portfolio value.  \n",
    "   - Reward:  \n",
    "     $$\n",
    "     r_t = - (V_{t+1} - V_t)^2\n",
    "     $$\n",
    "   - This mirrors the classical goal of delta hedging — keeping portfolio value as stable as possible.\n",
    "\n",
    "3. **Sharpe Ratio–Style Reward (Risk-Adjusted Return)**  \n",
    "   - Encourages smooth but profitable returns by rewarding return per unit of volatility.  \n",
    "   - Reward (approximate):  \n",
    "     $$\n",
    "     r_t = \\frac{V_{t+1} - V_t}{\\sigma_{rolling} + ϵ}\n",
    "     $$\n",
    "   - This is closer to how performance is evaluated in real-world trading.\n",
    "\n",
    "Each of these rewards will produce a different hedging behaviour, and in the next notebook we’ll compare their outcomes empirically using PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15d2ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run complete. Total reward: -761.1402139897922\n"
     ]
    }
   ],
   "source": [
    "# Model sanity check\n",
    "env = HedgingEnv()\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(\"Test run complete. Total reward:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
